{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview and implementation of [Confidential and Private Collaborative Learning](https://openreview.net/forum?id=h2EbJ4_wMVq). CaPC integrates cryptography and differential privacy to provide Confidential and Private Collaborative Learning. It is an extension of [PATE](https://arxiv.org/abs/1802.08908) and this relationship is shown more clearly in the diagram below. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"750\" alt=\"dpsgd\" src=\"http://cleverhans.io/assets/capc/capc1.PNG\">\n",
    "</p>\n",
    "\n",
    "Related files in this folder are referenced in this code and they can be opened for more details about the implementation. The MNIST dataset is used for this implementation however the code can be extended to other datasets as well. We divide the notebook into several sections based on the steps in the CaPC protocol. A brief description of the steps is first provided, which is then followed by the implementation. The numbering of the steps is the same as in the figure below. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"750\" alt=\"dpsgd\" src=\"http://cleverhans.io/assets/capc/capc2.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings for the number of parties and the index to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parties = 2 # Set the number of answering parties.\n",
    "index = 11 # Set the index of the data point in the MNIST test set to use as the query (index of a sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-74761f04ef12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclient_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dockuser/code/capc-privacy/utils/client_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtvds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "from utils import client_data\n",
    "from utils.time_utils import get_timestamp\n",
    "from utils.time_utils import log_timing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import atexit\n",
    "from utils.remove_files import remove_files_by_name\n",
    "import consts\n",
    "from consts import out_client_name\n",
    "from consts import out_server_name\n",
    "from consts import out_final_name\n",
    "import getpass\n",
    "\n",
    "import subprocess\n",
    "import client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments to be used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():    \n",
    "    user = getpass.getuser()\n",
    "    \"\"\"Initial setup of parameters to be used.\"\"\"\n",
    "    parser = argparse.ArgumentParser('')\n",
    "    parser.add_argument('--session', type=str, help='session name',\n",
    "                        default='capc')\n",
    "    parser.add_argument('--log_timing_file', type=str,\n",
    "                        help='name of the global log timing file',\n",
    "                        default=f'logs/log-timing-{get_timestamp()}.log')\n",
    "    parser.add_argument('--n_parties', type=int, default=n_parties,\n",
    "                        help='number of servers')\n",
    "    parser.add_argument('--start_port', type=int, default=37000,\n",
    "                        help='the number of the starting port')\n",
    "    parser.add_argument('--seed', type=int, default=2,\n",
    "                        help='seed for top level script')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--num_classes', type=int, default=10,\n",
    "                        help='Number of classes in the dataset.')\n",
    "    parser.add_argument(\n",
    "        \"--rstar_exp\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='The exponent for 2 to generate the random r* from.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_logit\",\n",
    "        type=float,\n",
    "        default=36.0,\n",
    "        help='The maximum value of a logit.',\n",
    "    )\n",
    "    parser.add_argument('--dp_noise_scale', type=float, default=0.05,\n",
    "                        help='The scale of the Gaussian noise for DP privacy.')\n",
    "    parser.add_argument(\n",
    "        \"--user\",\n",
    "        type=str,\n",
    "        default=user,\n",
    "        help=\"The name of the OS USER.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_level\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help='log level for he-transformer',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--round_exp',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='Multiply r* and logits by 2^round_exp.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_threads',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of threads.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--qp_id', type=int, default=0, help='which model is the QP?')\n",
    "    parser.add_argument(\n",
    "        \"--start_batch\",\n",
    "        type=int,\n",
    "        default=index,  # 0\n",
    "        help=\"Test data start index\")\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        type=str,\n",
    "        default='cryptonets-relu',\n",
    "        help=\"The type of models used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_node\",\n",
    "        type=str,\n",
    "        default=\"import/input:0\",\n",
    "        help=\"Tensor name of data input\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_node\",\n",
    "        type=str,\n",
    "        default=\"import/output/BiasAdd:0\",\n",
    "        help=\"Tensor name of model output\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dataset_path', type=str,\n",
    "        default='/home/dockuser/queries',\n",
    "        help='where the queries are.')\n",
    "    parser.add_argument(\n",
    "        '--dataset_name', type=str,\n",
    "        default='mnist',\n",
    "        help='name of dataset where queries came from')\n",
    "    parser.add_argument('--debug', default=False, action='store_true')\n",
    "    parser.add_argument('--n_queries',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='total len(queries)')\n",
    "    parser.add_argument('--checkpoint_dir', type=str,\n",
    "                        default=f'./models',\n",
    "                        help='dir with all checkpoints')\n",
    "    parser.add_argument('--cpu', default=False, action='store_true',\n",
    "                        help='set to use cpu and no encryption.')\n",
    "    parser.add_argument('--ignore_parties', default=True, action='store_true',\n",
    "                        # False\n",
    "                        help='set when using crypto models.')\n",
    "    # parser.add_argument('--',\n",
    "    #                     default='$HE_TRANSFORMER/configs/he_seal_ckks_config_N13_L5_gc.json')\n",
    "    parser.add_argument('--encryption_params',\n",
    "                        default='config/10.json')\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "    if unparsed:\n",
    "        print(\"Unparsed flags:\", unparsed)\n",
    "        exit(1)\n",
    "    return args\n",
    "\n",
    "def clean_old_files():\n",
    "    \"\"\"\n",
    "    Delete old data files.\n",
    "    This function is called before running the protocol.\n",
    "    \"\"\"\n",
    "    cur_dir = os.getcwd()\n",
    "    for name in [out_client_name,\n",
    "                 out_server_name,\n",
    "                 out_final_name,\n",
    "                 consts.input_data,\n",
    "                 consts.input_labels,\n",
    "                 consts.predict_labels,\n",
    "                 consts.label_final_name]:\n",
    "        remove_files_by_name(starts_with=name, directory=cur_dir)\n",
    "\n",
    "\n",
    "def delete_files(port):\n",
    "    \"\"\"\n",
    "    Delete files related to this port.\n",
    "    :param port: port number\n",
    "    \"\"\"\n",
    "    files_to_delete = [consts.out_client_name + str(port) + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_final_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_server_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [f\"{out_final_name}.txt\",\n",
    "                        f\"{out_server_name}.txt\"]  # aggregates across all parties\n",
    "    files_to_delete += [consts.inference_times_name,\n",
    "                        consts.argmax_times_name,\n",
    "                        consts.client_csp_times_name,\n",
    "                        consts.inference_no_network_times_name]\n",
    "    for f in files_to_delete:\n",
    "        if os.path.exists(f):\n",
    "            print(f'delete file: {f}')\n",
    "            os.remove(f)\n",
    "\n",
    "\n",
    "def set_data_labels(FLAGS):\n",
    "    \"\"\"Gets MNIST data and labels, saving it in the local folder\"\"\"\n",
    "    data, labels = get_data(start_batch=FLAGS.start_batch,\n",
    "                            batch_size=FLAGS.batch_size)\n",
    "    np.save(consts.input_data, data)\n",
    "    np.save(consts.input_labels, labels)\n",
    "\n",
    "\n",
    "def get_models(model_dir, n_parties, ignore_parties):\n",
    "    \"\"\"Gets model files from model_dir.\"\"\"\n",
    "    model_files = [f for f in os.listdir(model_dir) if\n",
    "                   os.path.isfile(os.path.join(model_dir, f))]\n",
    "    if len(model_files) != n_parties and not ignore_parties:\n",
    "        raise ValueError(\n",
    "            f'{len(model_files)} models found when {n_parties + 1} parties '\n",
    "            f'requested. Not equal.')\n",
    "    return model_dir, model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setup for the CaPC protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsed flags: ['-f', '/home/dockuser/.local/share/jupyter/runtime/kernel-7a337de6-2cb0-4ea2-abd9-1eb2e811085f.json']\n",
      "remove file:  input_data.npy\n",
      "remove file:  input_labels.npy\n",
      "delete file: files/logits37000privacy.txt\n",
      "delete file: files/output37000.txt\n",
      "delete file: files/noise37000.txt\n",
      "delete file: files/output.txt\n",
      "delete file: files/noise.txt\n",
      "delete file: files/inference_times\n",
      "delete file: files/argmax_times\n",
      "delete file: files/client_csp_times\n",
      "delete file: files/inference_no_network_times\n",
      "delete file: files/logits37001privacy.txt\n",
      "delete file: files/output37001.txt\n",
      "delete file: files/noise37001.txt\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "np.random.seed(args.seed)\n",
    "clean_old_files()\n",
    "set_data_labels(FLAGS=args)\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "log_timing_file = args.log_timing_file\n",
    "log_timing('main: start capc', log_file=log_timing_file)\n",
    "\n",
    "processes = []\n",
    "\n",
    "def kill_processes():\n",
    "    for p in processes:\n",
    "        p.kill()\n",
    "\n",
    "if not args.debug:\n",
    "    atexit.register(kill_processes)\n",
    "\n",
    "n_parties = args.n_parties\n",
    "n_queries = args.n_queries\n",
    "batch_size = args.batch_size\n",
    "num_classes = args.num_classes\n",
    "rstar_exp = args.rstar_exp\n",
    "log_level = args.log_level\n",
    "round_exp = args.round_exp\n",
    "num_threads = args.num_threads\n",
    "input_node = args.input_node\n",
    "output_node = args.output_node\n",
    "start_port = args.start_port\n",
    "index = args.start_batch\n",
    "\n",
    "# if FLAGS.cpu then use cpu without the encryption.\n",
    "backend = 'HE_SEAL' if not args.cpu else 'CPU'\n",
    "\n",
    "models_loc, model_files = get_models(\n",
    "    args.checkpoint_dir, n_parties=n_parties,\n",
    "    ignore_parties=args.ignore_parties)\n",
    "\n",
    "for port in range(start_port, start_port + n_queries * n_parties):\n",
    "    delete_files(port=port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The files server.py and client.py together complete Step 1 and can be referenced for more details. In this step, the querying party (in this case the client) first sends the query $q$ from the MNIST dataset to the answering party (the server) in Step 1a which on its own end generates a prediction for the query $r$. Each answering party then generates a random vector $r^{*}$ and  sends the vector $r-r^{*}$ to the querying party in Step 1b. Finally in Step 1c, the answering parties run  secure 2PC with the querying party to find the $s$ vector for the querying party and the $\\hat{s_i}$ vectors for the answering party so that $s + \\hat{s_i}$ is the one hot encoding of the argmax of the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port: 37000\n",
      "Start the servers (answering parties: APs).\n",
      "Start the client (the querying party: QP).\n",
      "port: 37001\n",
      "Start the servers (answering parties: APs).\n",
      "Start the client (the querying party: QP).\n"
     ]
    }
   ],
   "source": [
    "# Querying Process\n",
    "for query_num in range(n_queries):\n",
    "    for port, model_file in zip(\n",
    "                [start_port + int(i + query_num * n_parties) for i in\n",
    "                 range(n_parties)],\n",
    "                model_files):\n",
    "        print(f\"port: {port}\")\n",
    "        new_model_file = os.path.join(\n",
    "            \"/home/dockuser/models\", str(port) + \".pb\")\n",
    "\n",
    "        print('Start the servers (answering parties: APs).')\n",
    "        log_timing('start server (AP)', log_file=log_timing_file)\n",
    "        # Command to start server with the relevant parameters.\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                'python -W ignore', 'server.py',\n",
    "                '--backend', backend,\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--model_file', new_model_file,\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--indext', str(index),\n",
    "                '--encryption_parameters', args.encryption_params,\n",
    "                '--enable_client', 'true',\n",
    "                '--enable_gc', 'true',\n",
    "                '--mask_gc_inputs', 'true',\n",
    "                '--mask_gc_outputs', 'true',\n",
    "                '--from_pytorch', '1',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--dataset_path', args.dataset_path,\n",
    "                '--num_gc_threads', f'{num_threads}',\n",
    "                '--input_node', f'{input_node}',\n",
    "                '--output_node', f'{output_node}',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--rstar_exp', f'{rstar_exp}',\n",
    "                '--num_classes', f'{num_classes}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "                '--port', f'{port}',\n",
    "                '--checkpoint_dir', args.checkpoint_dir,\n",
    "            ])\n",
    "        server_process = subprocess.Popen(cmd_string, shell=True)\n",
    "        print(\"Start the client (the querying party: QP).\")\n",
    "        log_timing('start the client QP', log_file=log_timing_file)\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                # Command to start client server with the relevant parameters.\n",
    "                'python -W ignore client.py',\n",
    "                '--batch_size', f'{batch_size}',\n",
    "                '--encrypt_data_str', 'encrypt',\n",
    "                '--indext', str(index),\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--from_pytorch', '1',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--dataset_path', f'{args.dataset_path}',\n",
    "                '--port', f'{port}',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--data_partition', 'test',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "            ])\n",
    "        client_process = subprocess.Popen(cmd_string, shell=True)\n",
    "\n",
    "        client_process.wait()\n",
    "        server_process.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The file pg.py is used to run the privacy guardian (PG). The PG adds the $\\hat{s}$ vectors from all answering parties and then adds Gaussian noise. This is followed by 2PC between the PG and the querying party (who has the sum of the $s$ vectors) to compute the final label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start privacy guardian: python -W ignore pg.py --start_port 37000 --end_port 37002 --log_timing_file logs/log-timing-2021-11-04-22-43-39-583383.log --dp_noise_scale 0.05\n"
     ]
    }
   ],
   "source": [
    "log_timing('start privacy guardian', log_file=log_timing_file)\n",
    "# Command to run Privacy Guardian (Steps 2 & 3).\n",
    "cmd_string = \" \".join(\n",
    "    ['python -W ignore', 'pg.py',\n",
    "     '--start_port', f'{start_port + int(query_num * n_parties)}',\n",
    "     '--end_port',\n",
    "     f'{start_port + int(query_num * n_parties) + n_parties}',\n",
    "     '--log_timing_file', log_timing_file,\n",
    "     '--dp_noise_scale', str(args.dp_noise_scale),\n",
    "     ])\n",
    "print(f\"start privacy guardian: {cmd_string}\")\n",
    "pg_process = subprocess.Popen(cmd_string, shell=True)\n",
    "pg_process.wait()\n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare predicted label with actual label. The client (querying party) prints the outputted label. The actual label is manually found using the index of the query used. Note that the use of the client to run the function here is arbitary. The function can equivalently be made here below and used by calling print_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label():\n",
    "    \"\"\"Function to print final label after Step 3 i.e. 2PC is complete\"\"\"\n",
    "    with open(os.path.join('files', 'final_label.txt'), 'r') as file:\n",
    "        label = file.read(1)\n",
    "    print(\"Predicted label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  6\n",
      "The correct label should be:  6\n"
     ]
    }
   ],
   "source": [
    "print_label() \n",
    "#client.print_label()\n",
    "(x_train, y_train, x_test, y_test) = client_data.load_mnist_data(index, 1)\n",
    "print(\"The correct label should be: \", np.argmax(y_test)) \n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
