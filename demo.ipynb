{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview and implementation of [Confidential and Private Collaborative Learning](https://openreview.net/forum?id=h2EbJ4_wMVq). Related files in this folder are referenced in this code and they can be opened for more details about the implementation. The MNIST dataset is used for this implementation however the code can be extended to other datasets as well. We divide the notebook into several sections based on the steps in the CaPC protocol. A brief description of the step is first provided which is then followed by the implementation. The numbering of the steps is the same as in Figure 1 of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings for the number of parties and the index to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parties = 3 # Set the number of answering parties.\n",
    "index = 6 # Set the index of the mnist test set to use as the query (index of a sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script assumes that a subdir with name {n_parties} exists in /models with the model \n",
    "files stored here.\n",
    "The number of model files should equal the value of {n_parties} + 1.\n",
    "It kicks off a server for each answering party and a single client who will be \n",
    "requesting queries.\n",
    "client.py holds the clients training protocol, and server.py the response algorithms.\n",
    "train_inits.py should be run first to train each model on a separate partition and save \n",
    "them as per the required scheme.\n",
    "USAGE: call this file with: \n",
    "OMP_NUM_THREADS=24 NGRAPH_HE_VERBOSE_OPS=all NGRAPH_HE_LOG_LEVEL=3 python run_experiment.py\n",
    "SETUP: create a tmux session with 3 panes, each in /home/dockuser/code/capc\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "from utils import client_data\n",
    "from utils.client_data import get_data\n",
    "from utils.time_utils import get_timestamp, log_timing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import atexit\n",
    "from utils.remove_files import remove_files_by_name\n",
    "import consts\n",
    "from consts import out_client_name, out_server_name, out_final_name\n",
    "import getpass\n",
    "\n",
    "import subprocess\n",
    "import client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments to be used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():    \n",
    "    user = getpass.getuser()\n",
    "    \"\"\"Initial setup of parameters to be used.\"\"\"\n",
    "    parser = argparse.ArgumentParser('')\n",
    "    parser.add_argument('--session', type=str, help='session name',\n",
    "                        default='capc')\n",
    "    parser.add_argument('--log_timing_file', type=str,\n",
    "                        help='name of the global log timing file',\n",
    "                        default=f'logs/log-timing-{get_timestamp()}.log')\n",
    "    parser.add_argument('--n_parties', type=int, default=1,\n",
    "                        help='number of servers')\n",
    "    parser.add_argument('--start_port', type=int, default=37000,\n",
    "                        help='the number of the starting port')\n",
    "    parser.add_argument('--seed', type=int, default=2,\n",
    "                        help='seed for top level script')\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                        help='batch size')\n",
    "    parser.add_argument('--num_classes', type=int, default=10,\n",
    "                        help='Number of classes in the dataset.')\n",
    "    parser.add_argument(\n",
    "        \"--rstar_exp\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='The exponent for 2 to generate the random r* from.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_logit\",\n",
    "        type=float,\n",
    "        default=36.0,\n",
    "        help='The maximum value of a logit.',\n",
    "    )\n",
    "    parser.add_argument('--dp_noise_scale', type=float, default=0.1,\n",
    "                        help='The scale of the Gaussian noise for DP privacy.')\n",
    "    parser.add_argument(\n",
    "        \"--user\",\n",
    "        type=str,\n",
    "        default=user,\n",
    "        help=\"The name of the OS USER.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_level\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help='log level for he-transformer',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--round_exp',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='Multiply r* and logits by 2^round_exp.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_threads',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of threads.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--qp_id', type=int, default=0, help='which model is the QP?')\n",
    "    parser.add_argument(\n",
    "        \"--start_batch\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Test data start index\")\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        type=str,\n",
    "        default='cryptonets-relu',\n",
    "        help=\"The type of models used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_node\",\n",
    "        type=str,\n",
    "        default=\"import/input:0\",\n",
    "        help=\"Tensor name of data input\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_node\",\n",
    "        type=str,\n",
    "        default=\"import/output/BiasAdd:0\",\n",
    "        help=\"Tensor name of model output\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dataset_path', type=str,\n",
    "        default='/home/dockuser/queries',\n",
    "        help='where the queries are.')\n",
    "    parser.add_argument(\n",
    "        '--dataset_name', type=str,\n",
    "        default='mnist',\n",
    "        help='name of dataset where queries came from')\n",
    "    parser.add_argument('--debug', default=False, action='store_true')\n",
    "    parser.add_argument('--n_queries',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='total len(queries)')\n",
    "    parser.add_argument('--checkpoint_dir', type=str,\n",
    "                        default=f'./models',\n",
    "                        help='dir with all checkpoints')\n",
    "    parser.add_argument('--cpu', default=False, action='store_true',\n",
    "                        help='set to use cpu and no encryption.')\n",
    "    parser.add_argument('--ignore_parties', default=True, action='store_true',\n",
    "                        # False\n",
    "                        help='set when using crypto models.')\n",
    "    # parser.add_argument('--',\n",
    "    #                     default='$HE_TRANSFORMER/configs/he_seal_ckks_config_N13_L5_gc.json')\n",
    "    parser.add_argument('--encryption_params',\n",
    "                        default='config/10.json')\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "    if unparsed:\n",
    "        print(\"Unparsed flags:\", unparsed)\n",
    "        exit(1)\n",
    "    return args\n",
    "\n",
    "def clean_old_files():\n",
    "    \"\"\"\n",
    "    Delete old data files.\n",
    "    This function is called before running the protocol.\n",
    "    \"\"\"\n",
    "    cur_dir = os.getcwd()\n",
    "    for name in [out_client_name,\n",
    "                 out_server_name,\n",
    "                 out_final_name,\n",
    "                 consts.input_data,\n",
    "                 consts.input_labels,\n",
    "                 consts.predict_labels,\n",
    "                 consts.label_final_name]:\n",
    "        remove_files_by_name(starts_with=name, directory=cur_dir)\n",
    "\n",
    "\n",
    "def delete_files(port):\n",
    "    \"\"\"\n",
    "    Delete files related to this port.\n",
    "    :param port: port number\n",
    "    \"\"\"\n",
    "    files_to_delete = [consts.out_client_name + str(port) + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_final_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_server_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [f\"{out_final_name}.txt\",\n",
    "                        f\"{out_server_name}.txt\"]  # aggregates across all parties\n",
    "    files_to_delete += [consts.inference_times_name,\n",
    "                        consts.argmax_times_name,\n",
    "                        consts.client_csp_times_name,\n",
    "                        consts.inference_no_network_times_name]\n",
    "    for f in files_to_delete:\n",
    "        if os.path.exists(f):\n",
    "            print(f'delete file: {f}')\n",
    "            os.remove(f)\n",
    "\n",
    "\n",
    "def set_data_labels(FLAGS):\n",
    "    \"\"\"Gets MNIST data and labels, saving it in the local folder\"\"\"\n",
    "    data, labels = get_data(start_batch=FLAGS.start_batch,\n",
    "                            batch_size=FLAGS.batch_size)\n",
    "    np.save(consts.input_data, data)\n",
    "    np.save(consts.input_labels, labels)\n",
    "\n",
    "\n",
    "def get_models(model_dir, n_parties, ignore_parties):\n",
    "    \"\"\"Gets model files from model_dir.\"\"\"\n",
    "    model_files = [f for f in os.listdir(model_dir) if\n",
    "                   os.path.isfile(os.path.join(model_dir, f))]\n",
    "    if len(model_files) != n_parties and not ignore_parties:\n",
    "        raise ValueError(\n",
    "            f'{len(model_files)} models found when {n_parties + 1} parties '\n",
    "            f'requested. Not equal.')\n",
    "    return model_dir, model_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setup for CaPC protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unparsed flags: ['-f', '/home/dockuser/.local/share/jupyter/runtime/kernel-33b6b4db-5ebc-41ed-895c-fa235191cc1b.json']\n",
      "delete file: files/logits37000privacy.txt\n",
      "delete file: files/output37000.txt\n",
      "delete file: files/noise37000.txt\n",
      "delete file: files/output.txt\n",
      "delete file: files/noise.txt\n",
      "delete file: files/inference_times\n",
      "delete file: files/argmax_times\n",
      "delete file: files/client_csp_times\n",
      "delete file: files/inference_no_network_times\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "np.random.seed(args.seed)\n",
    "clean_old_files()\n",
    "set_data_labels(FLAGS=args)\n",
    "\n",
    "log_timing_file = args.log_timing_file\n",
    "    log_timing('main: start capc', log_file=log_timing_file)\n",
    "\n",
    "processes = []\n",
    "\n",
    "def kill_processes():\n",
    "    for p in processes:\n",
    "        p.kill()\n",
    "\n",
    "if not args.debug:\n",
    "    atexit.register(kill_processes)\n",
    "\n",
    "n_parties = args.n_parties\n",
    "n_queries = args.n_queries\n",
    "batch_size = args.batch_size\n",
    "num_classes = args.num_classes\n",
    "rstar_exp = args.rstar_exp\n",
    "log_level = args.log_level\n",
    "round_exp = args.round_exp\n",
    "num_threads = args.num_threads\n",
    "input_node = args.input_node\n",
    "output_node = args.output_node\n",
    "start_port = args.start_port\n",
    "index = args.start_batch\n",
    "\n",
    "# if FLAGS.cpu then use cpu without the encryption.\n",
    "backend = 'HE_SEAL' if not args.cpu else 'CPU'\n",
    "\n",
    "models_loc, model_files = get_models(\n",
    "    args.checkpoint_dir, n_parties=n_parties,\n",
    "    ignore_parties=args.ignore_parties)\n",
    "\n",
    "for port in range(start_port, start_port + n_queries * n_parties):\n",
    "    delete_files(port=port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The files server.py and client.py together complete Step 1 and can be referenced for more details. In this step, the querying party (in this case the client) first sends the query $q$ from the MNIST dataset to the answering party (the server) in Step 1a which on its own end generates a prediction for the query $r$. Each answering party then generates a random vector $r^{*}$ and  sends the vector $r-r^{*}$ to the querying party in Step 1b. Finally in Step 1c, the answering parties run  secure 2PC with the querying party to find the $s$ vector for the querying party and the $\\hat{s_i}$ vectors for the answering party so that $s + \\hat{s_i}$ is the one hot encoding of the argmax of the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port: 37000\n",
      "run_exp rstar: [ -95.08244041 -934.90307722  137.7087547   -96.45973992 -127.08674132\n",
      " -311.47428658 -568.87959749  280.26693909 -374.30722831 -441.53774059]\n",
      "port: 37000\n",
      "Start the servers (answering parties: APs)\n",
      "port: 37000\n",
      "port: 37001\n",
      "run_exp rstar: [ 284.08208951   95.68300908 -712.38027193   63.80799235 -610.26715516\n",
      "  620.36638273  760.94139933   24.19704296  745.75792201 -824.88606309]\n",
      "port: 37001\n",
      "Start the servers (answering parties: APs)\n",
      "port: 37001\n",
      "port: 37002\n",
      "run_exp rstar: [  46.74399257 -854.29323902 -111.20547308 -790.30468473 -727.57637796\n",
      "  234.13439279 -525.12742276 -768.97523853 -536.81288792 -271.55576831]\n",
      "port: 37002\n",
      "Start the servers (answering parties: APs)\n",
      "port: 37002\n"
     ]
    }
   ],
   "source": [
    "# Querying Process\n",
    "for query_num in range(n_queries):\n",
    "    for port, model_file in zip(\n",
    "                [start_port + int(i + query_num * n_parties) for i in\n",
    "                 range(n_parties)],\n",
    "                model_files):\n",
    "        print(f\"port: {port}\")\n",
    "        new_model_file = os.path.join(\n",
    "            \"/home/dockuser/models\", str(port) + \".pb\")\n",
    "\n",
    "        print('Start the servers (answering parties: APs).')\n",
    "        log_timing('start server (AP)', log_file=log_timing_file)\n",
    "        # Command to start server with the relevant parameters.\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                'python -W ignore', 'server.py',\n",
    "                '--backend', backend,\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--model_file', new_model_file,\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--indext', str(index),\n",
    "                '--encryption_parameters', args.encryption_params,\n",
    "                '--enable_client', 'true',\n",
    "                '--enable_gc', 'true',\n",
    "                '--mask_gc_inputs', 'true',\n",
    "                '--mask_gc_outputs', 'true',\n",
    "                '--from_pytorch', '1',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--dataset_path', args.dataset_path,\n",
    "                '--num_gc_threads', f'{num_threads}',\n",
    "                '--input_node', f'{input_node}',\n",
    "                '--output_node', f'{output_node}',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--rstar_exp', f'{rstar_exp}',\n",
    "                '--num_classes', f'{num_classes}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "                '--port', f'{port}',\n",
    "                '--checkpoint_dir', args.checkpoint_dir,\n",
    "            ])\n",
    "        server_process = subprocess.Popen(cmd_string, shell=True)\n",
    "        print(\"Start the client (the querying party: QP).\")\n",
    "        log_timing('start the client QP', log_file=log_timing_file)\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                # Command to start client server with the relevant parameters.\n",
    "                'python -W ignore client.py',\n",
    "                '--batch_size', f'{batch_size}',\n",
    "                '--encrypt_data_str', 'encrypt',\n",
    "                '--indext', str(index),\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--from_pytorch', '1',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--dataset_path', f'{args.dataset_path}',\n",
    "                '--port', f'{port}',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--data_partition', 'test',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "            ])\n",
    "        client_process = subprocess.Popen(cmd_string, shell=True)\n",
    "\n",
    "        client_process.wait()\n",
    "        server_process.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The file pg.py is used to run the privacy guardian (PG). The PG adds the $\\hat{s}$ vectors from all answering parties and then adds Gaussian noise. This is followed by 2PC between the PG and the querying party (who has the sum of the $s$ vectors) to compute the final label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start privacy guardian: python -W ignore pg.py 37000 37003\n"
     ]
    }
   ],
   "source": [
    "log_timing('start privacy guardian', log_file=log_timing_file)\n",
    "# Command to run Privacy Guardian (Steps 2 & 3).\n",
    "cmd_string = \" \".join(\n",
    "    ['python -W ignore', 'pg.py',\n",
    "     '--start_port', f'{start_port + int(query_num * n_parties)}',\n",
    "     '--end_port',\n",
    "     f'{start_port + int(query_num * n_parties) + n_parties}',\n",
    "     '--log_timing_file', log_timing_file,\n",
    "     '--dp_noise_scale', str(args.dp_noise_scale),\n",
    "     ])\n",
    "print(f\"start privacy guardian: {cmd_string}\")\n",
    "pg_process = subprocess.Popen(cmd_string, shell=True)\n",
    "pg_process.wait()\n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare predicted label with actual label. The client (querying party) prints the outputted label. The actual label is manually found using the index of the query used. Note that the use of the client to run the function here is arbitary. The function can equivalently be made here below and used by calling print_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label():\n",
    "    \"\"\"Function to print final label after Step 3 i.e. 2PC is complete\"\"\"\n",
    "    with open(f\"{label_final_name}.txt\", 'r') as file:\n",
    "        label = file.read(1)\n",
    "    logger = create_logger(save_path='logs', file_type='client')\n",
    "    logger.info(f\"Predicted label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  7\n",
      "Actual label 7\n"
     ]
    }
   ],
   "source": [
    "client.print_label() \n",
    "(x_train, y_train, x_test, y_test) = client_data.load_mnist_data(index, 1)\n",
    "print(\"The correct label should be: \", np.argmax(y_test)) \n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
