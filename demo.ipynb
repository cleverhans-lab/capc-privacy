{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview and implementation of [Confidential and Private Collaborative Learning](https://openreview.net/forum?id=h2EbJ4_wMVq). CaPC integrates cryptography and differential privacy to provide Confidential and Private Collaborative Learning. It is an extension of [PATE](https://arxiv.org/abs/1802.08908) and this relationship is shown more clearly in the diagram below. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"750\" alt=\"dpsgd\" src=\"http://cleverhans.io/assets/capc/capc1.PNG\">\n",
    "</p>\n",
    "\n",
    "Related files in this folder are referenced in this code and they can be opened for more details about the implementation. The MNIST dataset is used for this implementation however the code can be extended to other datasets as well. We divide the notebook into several sections based on the steps in the CaPC protocol. A brief description of the steps is first provided, which is then followed by the implementation. The numbering of the steps is the same as in the figure below. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"750\" alt=\"dpsgd\" src=\"http://cleverhans.io/assets/capc/capc2.PNG\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings for the number of parties and the index to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parties = 2 # Set the number of answering parties.\n",
    "index = 11 # Set the index of the data point in the MNIST test set to use as the query (index of a sample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from utils import client_data\n",
    "from utils.time_utils import get_timestamp\n",
    "from utils.time_utils import log_timing\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import atexit\n",
    "from utils.remove_files import remove_files_by_name\n",
    "import consts\n",
    "from consts import out_client_name\n",
    "from consts import out_server_name\n",
    "from consts import out_final_name\n",
    "import getpass\n",
    "\n",
    "import subprocess\n",
    "import client\n",
    "import server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments to be used in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from utils.time_utils import get_timestamp\n",
    "\n",
    "DEFAULT_PORT = 8000\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in (\"on\", \"yes\", \"true\", \"t\", \"y\", \"1\"):\n",
    "        return True\n",
    "    elif v.lower() in (\"off\", \"no\", \"false\", \"f\", \"n\", \"0\"):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n",
    "\n",
    "\n",
    "def argument_parser():\n",
    "    user = getpass.getuser()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=4567, help=\"random seed\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Batch size\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=1,\n",
    "                        help='Number of workers to fetch the data.')\n",
    "    parser.add_argument('--is_cuda', type=str2bool, default=True,\n",
    "                        help='Is CUDA training enabled?')\n",
    "    parser.add_argument('--final_call', type=int, default=0,\n",
    "                        help='always 0 unless final call to client.')\n",
    "    parser.add_argument('--encrypt_data_str', type=str, default=\"encrypt\")\n",
    "    parser.add_argument('--from_pytorch', type=int, default=1,\n",
    "                        help='set to 1 to use pytorch bridge')\n",
    "    parser.add_argument(\n",
    "        \"--backend\", type=str, default=\"HE_SEAL\", help=\"Name of backend to use\")\n",
    "    parser.add_argument(\n",
    "        \"--encryption_parameters\",\n",
    "        type=str,\n",
    "        # default=\"../../config/he_seal_ckks_config_N13_L4_gc_50.json\",\n",
    "        default='config/10.json',\n",
    "        help=\"Filename containing json description of encryption parameters, \"\n",
    "             \"or json description itself\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enable_client\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Enable the client\")\n",
    "    parser.add_argument(\n",
    "        \"--enable_gc\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Enable garbled circuits\")\n",
    "    parser.add_argument(\n",
    "        \"--mask_gc_inputs\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Mask garbled circuits inputs\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask_gc_outputs\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Mask garbled circuits outputs\",\n",
    "    )\n",
    "    parser.add_argument('--data_partition', type=str, default='test',\n",
    "                        choices=['train', 'test'],\n",
    "                        help='test or train partition.')\n",
    "    parser.add_argument(\n",
    "        \"--num_gc_threads\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"Number of threads to run garbled circuits with\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_node\",\n",
    "        type=str,\n",
    "        default=\"import/input:0\",  # input:0\n",
    "        help=\"Tensor name of data input\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_node\",\n",
    "        type=str,\n",
    "        default=\"import/output/BiasAdd:0\",  # local__model/dense_1/BiasAdd:0\n",
    "        help=\"Tensor name of model output\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--minibatch_id', type=int, default=0,\n",
    "        help='which index in the minibatch to work on.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--indext', type=int, default=0,\n",
    "        help='which index of the mnist test set to use as the query'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hostname\", type=str, default=\"localhost\", help=\"Hostname of server\")\n",
    "    parser.add_argument(\n",
    "        \"--pack_data\",\n",
    "        type=str2bool,\n",
    "        default=True,\n",
    "        help=\"Use plaintext packing on data\")\n",
    "    parser.add_argument(\n",
    "        \"--port\", type=int,\n",
    "        default=DEFAULT_PORT,\n",
    "        help=\"Ports of server\")\n",
    "    parser.add_argument(\n",
    "        \"--rstar_exp\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='The exponent for 2 to generate the random r* from.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of possible classes in the classification task.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--checkpoint_dir', type=str,\n",
    "        default=f'./models',\n",
    "        # default=f'./architecture/',\n",
    "        help='Path to the directory with all checkpoints.')\n",
    "\n",
    "    parser.add_argument('--n_parties', type=int, default=1)  # , required=True)\n",
    "    parser.add_argument(\n",
    "        '--r_star',\n",
    "        nargs='+',\n",
    "        type=float,\n",
    "        default=None,\n",
    "        help=\"\"\"For debug purposes: Each AP subtracts a vector of random \n",
    "        numbers r* from the logits r (this is done via the homomorphic \n",
    "        encryption). The encrypted result (r - r*) is sent back to the QP \n",
    "        (client). When QP decrypts the received result, it obtains (r - r*) in \n",
    "        plain text (note that this is not the plain result r). We can verify \n",
    "        that this was done correctly by computing (r - r*) + r* = r.\"\"\"\n",
    "    )\n",
    "    parser.add_argument('--max_logit', default=36.0, type=float,\n",
    "                        help='max logit found.')\n",
    "\n",
    "    parser.add_argument(\"--query_ids\", type=int, nargs='+')\n",
    "    parser.add_argument(\n",
    "        '--round_exp',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='Multiply r* and logits by 2^round_exp.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dataset_path', type=str, default=\"/home/dockuser/queries\", help='dataset to use.')\n",
    "    parser.add_argument('--dataset_name', type=str, default='mnist',\n",
    "                        help='name of dataset where queries came from.')\n",
    "    parser.add_argument('--log_timing_file', type=str,\n",
    "                        help='name of the global log timing file',\n",
    "                        default=f'logs/log-timing-{get_timestamp()}.log')\n",
    "    parser.add_argument('--debug', default=False, action='store_true')\n",
    "    parser.add_argument('--n_queries',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='total len(queries)')\n",
    "    parser.add_argument(\n",
    "        '--num_threads',\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help='Number of threads.',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--start_batch\",\n",
    "        type=int,\n",
    "        default=index,\n",
    "        help=\"Test data start index\")\n",
    "    parser.add_argument('--ignore_parties', default=True, action='store_true',\n",
    "                        # False\n",
    "                        help='set when using crypto models.')\n",
    "    parser.add_argument('--encryption_params',\n",
    "                        default='config/10.json')\n",
    "    parser.add_argument(\n",
    "        \"--log_level\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help='log level for he-transformer',\n",
    "    )\n",
    "    parser.add_argument('--start_port', type=int, default=37000,\n",
    "                        help='the number of the starting port')\n",
    "    return parser\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    args, unparsed = argument_parser().parse_known_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    args.is_cuda = args.is_cuda and torch.cuda.is_available()\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\" if args.is_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    args.device = device\n",
    "\n",
    "    return args\n",
    "\n",
    "def get_model_files(model_dir, n_parties, ignore_parties):\n",
    "    \"\"\"Get model files from model_dir.\"\"\"\n",
    "    model_files = [f for f in os.listdir(model_dir) if\n",
    "                   os.path.isfile(os.path.join(model_dir, f))]\n",
    "    if len(model_files) != n_parties and not ignore_parties:\n",
    "        raise ValueError(\n",
    "            f'{len(model_files)} models found when {n_parties + 1} parties '\n",
    "            f'requested. Not equal.')\n",
    "    return model_dir, model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_old_files():\n",
    "    \"\"\"\n",
    "    Delete old data files.\n",
    "    This function is called before running the protocol.\n",
    "    \"\"\"\n",
    "    cur_dir = os.getcwd()\n",
    "    for name in [out_client_name,\n",
    "                 out_server_name,\n",
    "                 out_final_name,\n",
    "                 consts.input_data,\n",
    "                 consts.input_labels,\n",
    "                 consts.predict_labels,\n",
    "                 consts.label_final_name]:\n",
    "        remove_files_by_name(starts_with=name, directory=cur_dir)\n",
    "\n",
    "\n",
    "def delete_files(port):\n",
    "    \"\"\"\n",
    "    Delete files related to this port.\n",
    "    :param port: port number\n",
    "    \"\"\"\n",
    "    files_to_delete = [consts.out_client_name + str(port) + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_final_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [\n",
    "        consts.out_server_name + str(port) + '.txt']  # + 'privacy.txt']\n",
    "    files_to_delete += [f\"{out_final_name}.txt\",\n",
    "                        f\"{out_server_name}.txt\"]  # aggregates across all parties\n",
    "    files_to_delete += [consts.inference_times_name,\n",
    "                        consts.argmax_times_name,\n",
    "                        consts.client_csp_times_name,\n",
    "                        consts.inference_no_network_times_name]\n",
    "    for f in files_to_delete:\n",
    "        if os.path.exists(f):\n",
    "            print(f'delete file: {f}')\n",
    "            os.remove(f)\n",
    "\n",
    "\n",
    "def set_data_labels(FLAGS):\n",
    "    \"\"\"Get MNIST data and labels, saving it in the local folder\"\"\"\n",
    "    data, labels = get_data(start_batch=FLAGS.start_batch,\n",
    "                            batch_size=FLAGS.batch_size)\n",
    "    np.save(consts.input_data, data)\n",
    "    np.save(consts.input_labels, labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setup for the CaPC protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.device:  cpu\n",
      "args.dataset_path:  /home/dockuser/queries\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "np.random.seed(args.seed)\n",
    "clean_old_files()\n",
    "\n",
    "if not os.path.exists('./logs'):\n",
    "    os.mkdir('./logs')\n",
    "    \n",
    "log_timing_file = args.log_timing_file\n",
    "log_timing('main: start capc', log_file=log_timing_file)\n",
    "\n",
    "processes = []\n",
    "\n",
    "def kill_processes():\n",
    "    for p in processes:\n",
    "        p.kill()\n",
    "\n",
    "if not args.debug:\n",
    "    atexit.register(kill_processes)\n",
    "\n",
    "n_parties = args.n_parties\n",
    "n_queries = args.n_queries\n",
    "batch_size = args.batch_size\n",
    "num_classes = args.num_classes\n",
    "rstar_exp = args.rstar_exp\n",
    "log_level = args.log_level\n",
    "round_exp = args.round_exp\n",
    "num_threads = args.num_threads\n",
    "input_node = args.input_node\n",
    "output_node = args.output_node\n",
    "start_port = args.start_port\n",
    "index = args.start_batch\n",
    "\n",
    "models_loc, model_files = get_model_files(\n",
    "    args.checkpoint_dir, n_parties=n_parties,\n",
    "    ignore_parties=args.ignore_parties)\n",
    "\n",
    "for port in range(start_port, start_port + n_queries * n_parties):\n",
    "    delete_files(port=port)\n",
    "\n",
    "print('args.device: ', args.device)\n",
    "print('args.dataset_path: ', args.dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MnistNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architecture.mnist_net import MnistNet\n",
    "\n",
    "def get_model(dataset: str, checkpoint_dir: str, device: str):\n",
    "    print('device: ', device)\n",
    "    model_file = os.path.join(checkpoint_dir, dataset, dataset + '.pt')\n",
    "    if dataset == 'mnist':\n",
    "        model = MnistNet()\n",
    "        if device == 'cuda' or device == torch.device('cuda'):\n",
    "            model = model.to(device)\n",
    "        model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "\n",
    "    return model\n",
    "\n",
    "get_model(dataset=args.dataset_name, checkpoint_dir=args.checkpoint_dir, device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The files server.py and client.py together complete Step 1 and can be referenced for more details. In this step, the querying party (in this case the client) first sends the query $q$ from the MNIST dataset to the answering party (the server) in Step 1a which on its own end generates a prediction for the query $r$. Each answering party then generates a random vector $r^{*}$ and  sends the vector $r-r^{*}$ to the querying party in Step 1b. Finally in Step 1c, the answering parties run  secure 2PC with the querying party to find the $s$ vector for the querying party and the $\\hat{s_i}$ vectors for the answering party so that $s + \\hat{s_i}$ is the one hot encoding of the argmax of the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "port: 37000\n",
      "Start the servers (answering parties: APs).\n",
      "Start the client (the querying party: QP).\n"
     ]
    }
   ],
   "source": [
    "# Querying process\n",
    "for query_num in range(n_queries):\n",
    "    for port, model_file in zip(\n",
    "            [start_port + int(i + query_num * n_parties) for i in\n",
    "             range(n_parties)],\n",
    "            model_files):\n",
    "        print(f\"port: {port}\")\n",
    "        new_model_file = os.path.join(\n",
    "            \"/home/dockuser/models\", str(port) + \".pb\")\n",
    "\n",
    "        print('Start the servers (answering parties: APs).')\n",
    "        log_timing('start server (AP)', log_file=log_timing_file)\n",
    "        # Command to start server with the relevant parameters.\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                'python -W ignore', 'server.py',\n",
    "                '--backend', backend,\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--model_file', new_model_file,\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--indext', str(index),\n",
    "                '--encryption_parameters', args.encryption_params,\n",
    "                '--enable_client', 'true',\n",
    "                '--enable_gc', 'true',\n",
    "                '--mask_gc_inputs', 'true',\n",
    "                '--mask_gc_outputs', 'true',\n",
    "                '--from_pytorch', '1',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--dataset_path', args.dataset_path,\n",
    "                '--num_gc_threads', f'{num_threads}',\n",
    "                '--input_node', f'{input_node}',\n",
    "                '--output_node', f'{output_node}',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--rstar_exp', f'{rstar_exp}',\n",
    "                '--num_classes', f'{num_classes}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "                '--port', f'{port}',\n",
    "                '--checkpoint_dir', args.checkpoint_dir,\n",
    "            ])\n",
    "        server_process = subprocess.Popen(cmd_string, shell=True)\n",
    "        print(\"Start the client (the querying party: QP).\")\n",
    "        log_timing('start the client QP', log_file=log_timing_file)\n",
    "        cmd_string = \" \".join(\n",
    "            [\n",
    "                # Command to start client server with the relevant parameters.\n",
    "                'python -W ignore client.py',\n",
    "                '--batch_size', f'{batch_size}',\n",
    "                '--encrypt_data_str', 'encrypt',\n",
    "                '--indext', str(index),\n",
    "                '--n_parties', f'{n_parties}',\n",
    "                '--round_exp', f'{round_exp}',\n",
    "                '--from_pytorch', '1',\n",
    "                '--minibatch_id', f'{query_num}',\n",
    "                '--dataset_path', f'{args.dataset_path}',\n",
    "                '--port', f'{port}',\n",
    "                '--dataset_name', args.dataset_name,\n",
    "                '--data_partition', 'test',\n",
    "                '--log_timing_file', log_timing_file,\n",
    "            ])\n",
    "        client_process = subprocess.Popen(cmd_string, shell=True)\n",
    "\n",
    "        client_process.wait()\n",
    "        server_process.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The file pg.py is used to run the privacy guardian (PG). The PG adds the $\\hat{s}$ vectors from all answering parties and then adds Gaussian noise. This is followed by 2PC between the PG and the querying party (who has the sum of the $s$ vectors) to compute the final label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_timing('start privacy guardian', log_file=log_timing_file)\n",
    "# Command to run Privacy Guardian (Steps 2 & 3).\n",
    "cmd_string = \" \".join(\n",
    "    ['python -W ignore', 'pg.py',\n",
    "     '--start_port', f'{start_port + int(query_num * n_parties)}',\n",
    "     '--end_port',\n",
    "     f'{start_port + int(query_num * n_parties) + n_parties}',\n",
    "     '--log_timing_file', log_timing_file,\n",
    "     '--dp_noise_scale', str(args.dp_noise_scale),\n",
    "     ])\n",
    "print(f\"start privacy guardian: {cmd_string}\")\n",
    "pg_process = subprocess.Popen(cmd_string, shell=True)\n",
    "pg_process.wait()\n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare predicted label with actual label. The client (querying party) prints the outputted label. The actual label is manually found using the index of the query used. Note that the use of the client to run the function here is arbitary. The function can equivalently be made here below and used by calling print_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_label():\n",
    "    \"\"\"Function to print final label after Step 3 i.e. 2PC is complete\"\"\"\n",
    "    with open(os.path.join('files', 'final_label.txt'), 'r') as file:\n",
    "        label = file.read(1)\n",
    "    print(\"Predicted label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_label() \n",
    "#client.print_label()\n",
    "(x_train, y_train, x_test, y_test) = client_data.load_mnist_data(index, 1)\n",
    "print(\"The correct label should be: \", np.argmax(y_test)) \n",
    "log_timing('finish capc', log_file=log_timing_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
